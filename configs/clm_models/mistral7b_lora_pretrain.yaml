_target_: src.models_clm.peft_models.get_peft_model_with_resize_embedding
model:
  _target_: src.models.mistral.modeling_mistral.MistralForCausalLM.from_pretrained
  pretrained_model_name_or_path: pretrained/Divot_pretrain/llm
  low_cpu_mem_usage: True
peft_config:
  _target_: peft.LoraConfig
  _convert_: object
  r: 32
  lora_alpha: 32
  modules_to_save:
    # - embed_tokens
    # - lm_head
    - input_layernorm
    - post_attention_layernorm
    - norm
  target_modules: 
    - q_proj 
    - v_proj 
    - k_proj 
    - o_proj 
    - gate_proj 
    - down_proj 
    - up_proj
  task_type: CAUSAL_LM
  lora_dropout: 0.05

vocab_size: 32262
